{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set the folder path where your Excel files are located\n",
    "folder_path = r\"C:\\Users\\34673\\Documents\\Universidad\\Master\\Florida Tech\\Neural Networks\\Project - Solar\\Datass\"  # Replace with your actual folder path\n",
    "\n",
    "# Dictionary to store period information for each file\n",
    "file_periods = {}\n",
    "all_periods = []\n",
    "\n",
    "# Regular expression to extract the period from cell A1\n",
    "period_pattern = r\"Period: (\\d{2}/\\d{2}/\\d{4}) to (\\d{2}/\\d{2}/\\d{4})\"\n",
    "\n",
    "# Function to convert date string to datetime object\n",
    "def convert_to_date(date_str):\n",
    "    return datetime.strptime(date_str, \"%d/%m/%Y\")\n",
    "\n",
    "# Function to convert datetime to string in the format we need\n",
    "def date_to_str(date_obj):\n",
    "    return date_obj.strftime(\"%d%m%Y\")\n",
    "\n",
    "# Step 1: Scan the folder and extract period information\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(('.xls', '.xlsx')):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        try:\n",
    "            # Try to read using pandas with default engine\n",
    "            df = pd.read_excel(file_path, nrows=1, usecols=[0], engine=None)\n",
    "            period_text = df.iloc[0, 0]\n",
    "            period_text = df.columns[0]\n",
    "                        \n",
    "            # Extract the period dates using regex\n",
    "            match = re.search(period_pattern, period_text)\n",
    "            if match:\n",
    "                start_date = match.group(1)\n",
    "                end_date = match.group(2)\n",
    "                \n",
    "                # Convert to datetime objects\n",
    "                start_dt = convert_to_date(start_date)\n",
    "                end_dt = convert_to_date(end_date)\n",
    "                \n",
    "                # Create a period identifier\n",
    "                period_id = f\"{str(start_dt)[:11]}_to_{str(end_dt)[:11]}\"\n",
    "                \n",
    "                # Store the period information\n",
    "                if period_id in file_periods:\n",
    "                    file_periods[period_id].append(filename)\n",
    "                else:\n",
    "                    file_periods[period_id] = [filename]\n",
    "                \n",
    "                all_periods.append((start_dt, end_dt))\n",
    "                print(f\"Processed {filename}: {start_date} to {end_date}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Step 2: Find duplicate periods\n",
    "duplicates = {period: files for period, files in file_periods.items() if len(files) > 1}\n",
    "\n",
    "# Step 3: Find missing periods\n",
    "# Sort periods by start date\n",
    "all_periods.sort()\n",
    "\n",
    "if all_periods:\n",
    "    # Get the earliest and latest dates\n",
    "    earliest_date = all_periods[0][0]\n",
    "    latest_date = all_periods[-1][1]\n",
    "    \n",
    "    # Determine the period length (assuming all periods have the same length)\n",
    "    # Using the first period as reference\n",
    "    first_period_length = (all_periods[0][1] - all_periods[0][0]).days + 1\n",
    "    \n",
    "    # Generate all expected periods\n",
    "    expected_periods = []\n",
    "    current_date = earliest_date\n",
    "    \n",
    "    while current_date <= latest_date:\n",
    "        end_date = current_date + timedelta(days=first_period_length - 1)\n",
    "        expected_periods.append((current_date, end_date))\n",
    "        current_date = end_date + timedelta(days=1)\n",
    "    \n",
    "    # Find missing periods\n",
    "    missing_periods = []\n",
    "    for period in expected_periods:\n",
    "        if period not in all_periods:\n",
    "            missing_periods.append(period)\n",
    "\n",
    "# Step 4: Rename the files\n",
    "for period_id, filenames in file_periods.items():\n",
    "    for i, filename in enumerate(filenames):\n",
    "        old_path = os.path.join(folder_path, filename)\n",
    "        new_filename = f\"{period_id}.xls\"\n",
    "        \n",
    "        # Add a suffix if there are duplicates\n",
    "        if len(filenames) > 1:\n",
    "            new_filename = f\"{period_id}_copy{i+1}.xls\"\n",
    "        \n",
    "        new_path = os.path.join(folder_path, new_filename)\n",
    "        \n",
    "        try:\n",
    "            os.rename(old_path, new_path)\n",
    "            print(f\"Renamed: {filename} -> {new_filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error renaming {filename}: {e}\")\n",
    "\n",
    "# Print summary\n",
    "print(\"\\nSummary:\")\n",
    "print(f\"Total files processed: {len(file_periods)}\")\n",
    "print(f\"Earliest period: {earliest_date.strftime('%d/%m/%Y')} to {all_periods[0][1].strftime('%d/%m/%Y')}\")\n",
    "print(f\"Latest period: {all_periods[-1][0].strftime('%d/%m/%Y')} to {latest_date.strftime('%d/%m/%Y')}\")\n",
    "\n",
    "if duplicates:\n",
    "    print(\"\\nDuplicate periods found:\")\n",
    "    for period, files in duplicates.items():\n",
    "        print(f\"  {period}: {len(files)} files\")\n",
    "\n",
    "if missing_periods:\n",
    "    print(\"\\nMissing periods:\")\n",
    "    for start, end in missing_periods:\n",
    "        print(f\"  {start.strftime('%d/%m/%Y')} to {end.strftime('%d/%m/%Y')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers from first file: DateTime, Most recent forecast [MW], Day-Ahead forecast [MW], Week-Ahead forecast [MW], Real-time Upscaled Measurement [MW], Corrected Upscaled Measurement [MW], Monitored Capacity [MWp], Day-Ahead forecast (11h00) [MW]\n",
      "Processed: 2019-01-01 _to_2019-01-31 .xls - 2976 rows\n",
      "Processed: 2019-02-01 _to_2019-02-28 .xls - 2688 rows\n",
      "Processed: 2019-03-01 _to_2019-03-31 .xls - 2972 rows\n",
      "Processed: 2019-04-01 _to_2019-04-30 .xls - 2880 rows\n",
      "Processed: 2019-05-01 _to_2019-05-31 .xls - 2976 rows\n",
      "Processed: 2019-06-01 _to_2019-06-30 .xls - 2880 rows\n",
      "Processed: 2019-07-01 _to_2019-07-31 .xls - 2976 rows\n",
      "Processed: 2019-08-01 _to_2019-08-31 .xls - 2976 rows\n",
      "Processed: 2019-09-01 _to_2019-09-30 .xls - 2880 rows\n",
      "Processed: 2019-10-01 _to_2019-10-31 .xls - 2980 rows\n",
      "Processed: 2019-11-01 _to_2019-11-30 .xls - 2880 rows\n",
      "Processed: 2019-12-01 _to_2019-12-31 .xls - 2976 rows\n",
      "Processed: 2020-01-01 _to_2020-01-31 .xls - 2976 rows\n",
      "Processed: 2020-02-01 _to_2020-02-29 .xls - 2784 rows\n",
      "Processed: 2020-03-01 _to_2020-03-31 .xls - 2972 rows\n",
      "Processed: 2020-04-01 _to_2020-04-30 .xls - 2880 rows\n",
      "Processed: 2020-05-01 _to_2020-05-31 .xls - 2976 rows\n",
      "Processed: 2020-06-01 _to_2020-06-30 .xls - 2880 rows\n",
      "Processed: 2020-07-01 _to_2020-07-31 .xls - 2976 rows\n",
      "Processed: 2020-08-01 _to_2020-08-31 .xls - 2976 rows\n",
      "Processed: 2020-09-01 _to_2020-09-30 .xls - 2880 rows\n",
      "Processed: 2020-10-01 _to_2020-10-31 .xls - 2980 rows\n",
      "Processed: 2020-11-01 _to_2020-11-30 .xls - 2880 rows\n",
      "Processed: 2020-12-01 _to_2020-12-31 .xls - 2976 rows\n",
      "Processed: 2021-01-01 _to_2021-01-31 .xls - 2976 rows\n",
      "Processed: 2021-02-01 _to_2021-02-28 .xls - 2688 rows\n",
      "Processed: 2021-03-01 _to_2021-03-31 .xls - 2972 rows\n",
      "Processed: 2021-04-01 _to_2021-04-30 .xls - 2880 rows\n",
      "Processed: 2021-05-01 _to_2021-05-31 .xls - 2976 rows\n",
      "Processed: 2021-06-01 _to_2021-06-30 .xls - 2880 rows\n",
      "Processed: 2021-07-01 _to_2021-07-31 .xls - 2976 rows\n",
      "Processed: 2021-08-01 _to_2021-08-31 .xls - 2976 rows\n",
      "Processed: 2021-09-01 _to_2021-09-30 .xls - 2880 rows\n",
      "Processed: 2021-10-01 _to_2021-10-31 .xls - 2980 rows\n",
      "Processed: 2021-11-01 _to_2021-11-30 .xls - 2880 rows\n",
      "Processed: 2021-12-01 _to_2021-12-31 .xls - 2976 rows\n",
      "Processed: 2022-01-01 _to_2022-01-31 .xls - 2976 rows\n",
      "Processed: 2022-02-01 _to_2022-02-28 .xls - 2688 rows\n",
      "Processed: 2022-03-01 _to_2022-03-31 .xls - 2972 rows\n",
      "Processed: 2022-04-01 _to_2022-04-30 .xls - 2880 rows\n",
      "Processed: 2022-05-01 _to_2022-05-31 .xls - 2976 rows\n",
      "Processed: 2022-06-01 _to_2022-06-30 .xls - 2880 rows\n",
      "Processed: 2022-07-01 _to_2022-07-31 .xls - 2976 rows\n",
      "Processed: 2022-08-01 _to_2022-08-31 .xls - 2976 rows\n",
      "Processed: 2022-09-01 _to_2022-09-30 .xls - 2880 rows\n",
      "Processed: 2022-10-01 _to_2022-10-31 .xls - 2980 rows\n",
      "Processed: 2022-11-01 _to_2022-11-30 .xls - 2880 rows\n",
      "Processed: 2022-12-01 _to_2022-12-31 .xls - 2976 rows\n",
      "Processed: 2023-01-01 _to_2023-01-31 .xls - 2976 rows\n",
      "Processed: 2023-02-01 _to_2023-02-28 .xls - 2688 rows\n",
      "Processed: 2023-03-01 _to_2023-03-31 .xls - 2972 rows\n",
      "Processed: 2023-04-01 _to_2023-04-30 .xls - 2880 rows\n",
      "Processed: 2023-05-01 _to_2023-05-31 .xls - 2976 rows\n",
      "Processed: 2023-06-01 _to_2023-06-30 .xls - 2880 rows\n",
      "Processed: 2023-07-01 _to_2023-07-31 .xls - 2976 rows\n",
      "Processed: 2023-08-01 _to_2023-08-31 .xls - 2976 rows\n",
      "Processed: 2023-09-01 _to_2023-09-30 .xls - 2880 rows\n",
      "Processed: 2023-10-01 _to_2023-10-31 .xls - 2980 rows\n",
      "Processed: 2023-11-01 _to_2023-11-30 .xls - 2880 rows\n",
      "Processed: 2023-12-01 _to_2023-12-31 .xls - 2976 rows\n",
      "Processed: 2024-01-01 _to_2024-01-31 .xls - 2976 rows\n",
      "Processed: 2024-02-01 _to_2024-02-29 .xls - 2784 rows\n",
      "Processed: 2024-03-01 _to_2024-03-31 .xls - 2972 rows\n",
      "Processed: 2024-04-01 _to_2024-04-30 .xls - 2880 rows\n",
      "Processed: 2024-05-01 _to_2024-05-31 .xls - 2976 rows\n",
      "Processed: 2024-06-01 _to_2024-06-30 .xls - 2880 rows\n",
      "Processed: 2024-07-01 _to_2024-07-31 .xls - 2976 rows\n",
      "Processed: 2024-08-01 _to_2024-08-31 .xls - 2976 rows\n",
      "Processed: 2024-09-01 _to_2024-09-30 .xls - 2880 rows\n",
      "Processed: 2024-10-01 _to_2024-10-31 .xls - 2980 rows\n",
      "Processed: 2024-11-01 _to_2024-11-30 .xls - 2880 rows\n",
      "Processed: 2024-12-01 _to_2024-12-31 .xls - 2976 rows\n",
      "Processed: 2025-01-01 _to_2025-01-31 .xls - 2976 rows\n",
      "Processed: 2025-02-01 _to_2025-02-28 .xls - 2688 rows\n",
      "Processed: 2025-03-01 _to_2025-03-31 .xls - 2972 rows\n",
      "\n",
      "Combined data saved to: C:\\Users\\34673\\Documents\\Universidad\\Master\\Florida Tech\\Neural Networks\\Project - Solar\\Datass\\combined_data.xlsx\n",
      "Total rows: 219068\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Set the folder path where your .xls files are located\n",
    "  # Replace with your actual folder path\n",
    "\n",
    "# Set the output file path\n",
    "output_file = os.path.join(folder_path, \"combined_data.xlsx\")\n",
    "\n",
    "# Get a list of all .xls files in the folder\n",
    "excel_files = glob.glob(os.path.join(folder_path, \"*.xls\"))\n",
    "\n",
    "# Sort the files by name to maintain order\n",
    "excel_files.sort()\n",
    "\n",
    "# Create an empty list to store DataFrames\n",
    "all_data = []\n",
    "\n",
    "# Process each Excel file\n",
    "for i, file_path in enumerate(excel_files):\n",
    "    try:\n",
    "        # Read the Excel file, skipping the first 3 rows\n",
    "        # The 4th row (index 3) contains column headers\n",
    "        if i == 0:\n",
    "            # For the first file, read the headers (row 4) and use them\n",
    "            df = pd.read_excel(file_path, header=3)\n",
    "            print(f\"Headers from first file: {', '.join(df.columns.tolist())}\")\n",
    "        else:\n",
    "            # For subsequent files, skip headers and use the ones from the first file\n",
    "            df = pd.read_excel(file_path, header=3)\n",
    "        \n",
    "        # Add the DataFrame to our list\n",
    "        all_data.append(df)\n",
    "        print(f\"Processed: {os.path.basename(file_path)} - {len(df)} rows\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "\n",
    "# Check if we have any data\n",
    "if not all_data:\n",
    "    print(\"No data was processed. Check your file path and file formats.\")\n",
    "else:\n",
    "    # Combine all DataFrames into one\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Save the combined data to a new Excel file\n",
    "    combined_df.to_excel(output_file, index=False)\n",
    "    \n",
    "    print(f\"\\nCombined data saved to: {output_file}\")\n",
    "    print(f\"Total rows: {len(combined_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## METEO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           date  cloud_cover  cloud_cover_low  \\\n",
      "0     2018-12-31 22:00:00+00:00        6.324             97.0   \n",
      "1     2018-12-31 23:00:00+00:00        6.024             99.0   \n",
      "2     2019-01-01 00:00:00+00:00        6.174             97.0   \n",
      "3     2019-01-01 01:00:00+00:00        6.324            100.0   \n",
      "4     2019-01-01 02:00:00+00:00        6.274            100.0   \n",
      "...                         ...          ...              ...   \n",
      "54763 2025-03-31 17:00:00+00:00       10.774              4.0   \n",
      "54764 2025-03-31 18:00:00+00:00        8.824              7.0   \n",
      "54765 2025-03-31 19:00:00+00:00        7.524              3.0   \n",
      "54766 2025-03-31 20:00:00+00:00        6.824              0.0   \n",
      "54767 2025-03-31 21:00:00+00:00        5.974              1.0   \n",
      "\n",
      "       cloud_cover_mid  cloud_cover_high  temperature_2m  \\\n",
      "0                 96.0               0.0            30.0   \n",
      "1                 93.0               0.0            80.0   \n",
      "2                100.0               0.0            88.0   \n",
      "3                100.0               0.0            87.0   \n",
      "4                100.0               0.0            80.0   \n",
      "...                ...               ...             ...   \n",
      "54763              4.0               0.0             0.0   \n",
      "54764              7.0               0.0             0.0   \n",
      "54765              3.0               0.0             0.0   \n",
      "54766              0.0               0.0             0.0   \n",
      "54767              1.0               0.0             0.0   \n",
      "\n",
      "       precipitation_probability  weather_code  \n",
      "0                            NaN           3.0  \n",
      "1                            NaN           3.0  \n",
      "2                            NaN           3.0  \n",
      "3                            NaN           3.0  \n",
      "4                            NaN           3.0  \n",
      "...                          ...           ...  \n",
      "54763                        NaN           0.0  \n",
      "54764                        NaN           0.0  \n",
      "54765                        NaN           0.0  \n",
      "54766                        NaN           0.0  \n",
      "54767                        NaN           0.0  \n",
      "\n",
      "[54768 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "from retry_requests import retry\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Specify coordinates for the location you're interested in\n",
    "latitude = 50.6403  # Example: Berlin\n",
    "longitude = 4.6667\n",
    "\n",
    "# Define the API request parameters\n",
    "url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": \"2019-01-01\",  # Adjust to your desired date range\n",
    "    \"end_date\": \"2025-03-31\",\n",
    "    \"hourly\": [\"temperature_2m\", \"cloud_cover\", \"cloud_cover_low\", \"cloud_cover_mid\", \"cloud_cover_high\", \"precipitation_probability\", \"weather_code\"],\n",
    "    \"timezone\": \"auto\"\n",
    "}\n",
    "\n",
    "# Make the API request\n",
    "responses = openmeteo.weather_api(url, params=params)\n",
    "response = responses[0]\n",
    "\n",
    "# Process hourly data\n",
    "hourly = response.Hourly()\n",
    "hourly_cloud_cover = hourly.Variables(0).ValuesAsNumpy()\n",
    "hourly_cloud_cover_low = hourly.Variables(1).ValuesAsNumpy()\n",
    "hourly_cloud_cover_mid = hourly.Variables(2).ValuesAsNumpy()\n",
    "hourly_cloud_cover_high = hourly.Variables(3).ValuesAsNumpy()\n",
    "hourly_temperature_2m = hourly.Variables(4).ValuesAsNumpy()\n",
    "hourly_precipitation_probability = hourly.Variables(5).ValuesAsNumpy()\n",
    "hourly_weather_code = hourly.Variables(6).ValuesAsNumpy()\n",
    "\n",
    "hourly_data = {\"date\": pd.date_range(\n",
    "\tstart = pd.to_datetime(hourly.Time(), unit = \"s\", utc = True),\n",
    "\tend = pd.to_datetime(hourly.TimeEnd(), unit = \"s\", utc = True),\n",
    "\tfreq = pd.Timedelta(seconds = hourly.Interval()),\n",
    "\tinclusive = \"left\"\n",
    ")}\n",
    "\n",
    "hourly_data[\"cloud_cover\"] = hourly_cloud_cover\n",
    "hourly_data[\"cloud_cover_low\"] = hourly_cloud_cover_low\n",
    "hourly_data[\"cloud_cover_mid\"] = hourly_cloud_cover_mid\n",
    "hourly_data[\"cloud_cover_high\"] = hourly_cloud_cover_high\n",
    "hourly_data[\"temperature_2m\"] = hourly_temperature_2m\n",
    "hourly_data[\"precipitation_probability\"] = hourly_precipitation_probability\n",
    "hourly_data[\"weather_code\"] = hourly_weather_code\n",
    "\n",
    "hourly_dataframe = pd.DataFrame(data = hourly_data)\n",
    "print(hourly_dataframe)\n",
    "\n",
    "\n",
    "# Save to CSV (optional)\n",
    "hourly_dataframe.to_csv(\"weather_data__in_Belgium.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cloud_cover</th>\n",
       "      <th>cloud_cover_low</th>\n",
       "      <th>cloud_cover_mid</th>\n",
       "      <th>cloud_cover_high</th>\n",
       "      <th>temperature_2m</th>\n",
       "      <th>precipitation_probability</th>\n",
       "      <th>weather_code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-12-31 22:00:00+00:00</td>\n",
       "      <td>6.324</td>\n",
       "      <td>97.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2018-12-31 23:00:00+00:00</td>\n",
       "      <td>6.024</td>\n",
       "      <td>99.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:00:00+00:00</td>\n",
       "      <td>6.174</td>\n",
       "      <td>97.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 01:00:00+00:00</td>\n",
       "      <td>6.324</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 02:00:00+00:00</td>\n",
       "      <td>6.274</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54763</th>\n",
       "      <td>2025-03-31 17:00:00+00:00</td>\n",
       "      <td>10.774</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54764</th>\n",
       "      <td>2025-03-31 18:00:00+00:00</td>\n",
       "      <td>8.824</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54765</th>\n",
       "      <td>2025-03-31 19:00:00+00:00</td>\n",
       "      <td>7.524</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54766</th>\n",
       "      <td>2025-03-31 20:00:00+00:00</td>\n",
       "      <td>6.824</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54767</th>\n",
       "      <td>2025-03-31 21:00:00+00:00</td>\n",
       "      <td>5.974</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54768 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            date  cloud_cover  cloud_cover_low  \\\n",
       "0      2018-12-31 22:00:00+00:00        6.324             97.0   \n",
       "1      2018-12-31 23:00:00+00:00        6.024             99.0   \n",
       "2      2019-01-01 00:00:00+00:00        6.174             97.0   \n",
       "3      2019-01-01 01:00:00+00:00        6.324            100.0   \n",
       "4      2019-01-01 02:00:00+00:00        6.274            100.0   \n",
       "...                          ...          ...              ...   \n",
       "54763  2025-03-31 17:00:00+00:00       10.774              4.0   \n",
       "54764  2025-03-31 18:00:00+00:00        8.824              7.0   \n",
       "54765  2025-03-31 19:00:00+00:00        7.524              3.0   \n",
       "54766  2025-03-31 20:00:00+00:00        6.824              0.0   \n",
       "54767  2025-03-31 21:00:00+00:00        5.974              1.0   \n",
       "\n",
       "       cloud_cover_mid  cloud_cover_high  temperature_2m  \\\n",
       "0                 96.0               0.0            30.0   \n",
       "1                 93.0               0.0            80.0   \n",
       "2                100.0               0.0            88.0   \n",
       "3                100.0               0.0            87.0   \n",
       "4                100.0               0.0            80.0   \n",
       "...                ...               ...             ...   \n",
       "54763              4.0               0.0             0.0   \n",
       "54764              7.0               0.0             0.0   \n",
       "54765              3.0               0.0             0.0   \n",
       "54766              0.0               0.0             0.0   \n",
       "54767              1.0               0.0             0.0   \n",
       "\n",
       "       precipitation_probability  weather_code  \n",
       "0                            NaN           3.0  \n",
       "1                            NaN           3.0  \n",
       "2                            NaN           3.0  \n",
       "3                            NaN           3.0  \n",
       "4                            NaN           3.0  \n",
       "...                          ...           ...  \n",
       "54763                        NaN           0.0  \n",
       "54764                        NaN           0.0  \n",
       "54765                        NaN           0.0  \n",
       "54766                        NaN           0.0  \n",
       "54767                        NaN           0.0  \n",
       "\n",
       "[54768 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_met = pd.read_csv('weather_data__in_Belgium.csv', delimiter=',')\n",
    "display(df_met)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from pysolar.solar import get_altitude, get_azimuth\n",
    "\n",
    "def get_sun_angles_for_day(latitude, longitude, date=None, local_timezone=None, interval_minutes=15):\n",
    "    \"\"\"\n",
    "    Calculate sun angles for an entire day at specified intervals using Pysolar.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    latitude : float\n",
    "        Latitude in degrees (positive for North, negative for South)\n",
    "    longitude : float\n",
    "        Longitude in degrees (positive for East, negative for West)\n",
    "    date : datetime, optional\n",
    "        Base date for calculations (time component will be ignored)\n",
    "    local_timezone : str, optional\n",
    "        Timezone name (e.g., 'America/New_York')\n",
    "    interval_minutes : int, optional\n",
    "        Interval between calculations in minutes. Default is 15.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    times : list\n",
    "        List of datetime objects\n",
    "    altitudes : numpy array\n",
    "        Sun altitude angles in degrees\n",
    "    azimuths : numpy array\n",
    "        Sun azimuth angles in degrees\n",
    "    \"\"\"\n",
    "    # Set up timezone\n",
    "    if local_timezone:\n",
    "        tz = pytz.timezone(local_timezone)\n",
    "    else:\n",
    "        tz = pytz.UTC\n",
    "    \n",
    "    # If no date provided, use today\n",
    "    if date is None:\n",
    "        date = datetime.now(tz)\n",
    "    elif not date.tzinfo:\n",
    "        date = tz.localize(date)\n",
    "    \n",
    "    # Strip time component to get just the date\n",
    "    date = date.replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    \n",
    "    # Calculate number of intervals in a day\n",
    "    intervals_per_day = 24 * 60 // interval_minutes\n",
    "    \n",
    "    # Create time points for the entire day\n",
    "    times = [date + timedelta(minutes=i * interval_minutes) for i in range(intervals_per_day)]\n",
    "    \n",
    "    # Convert times to UTC for Pysolar calculations\n",
    "    utc_times = [t.astimezone(pytz.UTC) for t in times]\n",
    "    \n",
    "    # Calculate sun position for each time point\n",
    "    altitudes = []\n",
    "    azimuths = []\n",
    "    \n",
    "    for t in utc_times:\n",
    "        # Pysolar expects UTC time\n",
    "        alt = get_altitude(latitude, longitude, t)\n",
    "        az = get_azimuth(latitude, longitude, t)\n",
    "        altitudes.append(alt)\n",
    "        azimuths.append(az)\n",
    "    \n",
    "    return times, np.array(altitudes), np.array(azimuths)\n",
    "\n",
    "def main():\n",
    "    # Example location: New York City\n",
    "    latitude = 50.6403\n",
    "    longitude = 4.6667\n",
    "    timezone_str = 'Europe/Madrid'  # Use None for UTC\n",
    "\n",
    "    tempos, altituds, azmuths = [], [], []\n",
    "    \n",
    "    # Get today's date (or specify a particular date)\n",
    "    date_1 = datetime(2019, 1, 1)\n",
    "    date_2 = datetime(2025, 3, 31)\n",
    "\n",
    "    d_day = date_1\n",
    "    while d_day <= date_2:\n",
    "        # Calculate sun angles for the day\n",
    "        times, altitudes, azimuths = get_sun_angles_for_day(\n",
    "            latitude, longitude, date=d_day, local_timezone=timezone_str)\n",
    "        d_day += timedelta(days=1)\n",
    "\n",
    "        tempos.extend(times)\n",
    "        altituds.extend(altitudes)\n",
    "        azmuths.extend(azimuths)\n",
    "    \n",
    "    # Create a pandas DataFrame with the results\n",
    "    results = pd.DataFrame({\n",
    "        'Time': tempos,\n",
    "        'Altitude (degrees)': altituds,\n",
    "        'Azimuths': azmuths\n",
    "    })\n",
    "    \n",
    "    # Return the vectors for altitude and azimuth\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('Sun_angles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data\n",
    "x['Altitude (degrees)'] = [0 if alt < 0 else alt for alt in data['Altitude (degrees)']]\n",
    "x.head(-20)\n",
    "x.to_csv('Solar_angle.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
